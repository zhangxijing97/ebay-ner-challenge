{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6f6a822-cba2-4fde-91bd-09669fd5fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & tokenizer ---\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "TOKENIZER = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER, use_fast=True)\n",
    "MAX_LEN = 160  # adjust if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e91c238-17e1-4630-929e-80317efaae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build stable label maps (ensure 'O' exists) ---\n",
    "def build_label_maps(df: pd.DataFrame):\n",
    "    tags = df[\"BIO_Tag\"].dropna().astype(str)\n",
    "    uniq = [\"O\"] + sorted(t for t in tags.unique().tolist() if t != \"O\")\n",
    "    label2id = {t: i for i, t in enumerate(uniq)}\n",
    "    id2label = {i: t for t, i in label2id.items()}\n",
    "    return label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d6d7eb-dd92-4e4e-b876-3b1b41eb13a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tokenize one title (group) and align word-level BIO to subwords ---\n",
    "def tokenize_align_group(group: pd.DataFrame, label2id: dict, max_len: int = MAX_LEN):\n",
    "    words  = group[\"Token\"].astype(str).tolist()\n",
    "    rnum   = group[\"Record Number\"].iloc[0]\n",
    "\n",
    "    enc = tokenizer(\n",
    "        words,\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    word_ids = enc.word_ids()\n",
    "    sub_tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"])\n",
    "\n",
    "    bio = group[\"BIO_Tag\"].astype(str).tolist()\n",
    "    rows = []\n",
    "    prev = None\n",
    "    for pos, wid in enumerate(word_ids):\n",
    "        # special tokens (None) or continuation subwords -> ignore in loss with -100\n",
    "        if wid is None:\n",
    "            label_id = -100\n",
    "            label_txt = \"-100\"\n",
    "        elif wid != prev:\n",
    "            label_txt = bio[wid] if bio[wid] and bio[wid] != \"nan\" else \"O\"\n",
    "            label_id = label2id.get(label_txt, label2id[\"O\"])\n",
    "        else:\n",
    "            label_txt = \"-100\"\n",
    "            label_id  = -100\n",
    "\n",
    "        rows.append({\n",
    "            \"Record Number\": rnum,\n",
    "            \"Subword\": sub_tokens[pos],\n",
    "            \"WordID\": -1 if wid is None else wid,\n",
    "            \"BIO_Label\": label_txt,\n",
    "            \"Label_ID\": label_id,\n",
    "            \"Input_ID\": enc[\"input_ids\"][pos]\n",
    "        })\n",
    "        prev = wid\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3670c622-2bc0-4bde-bea8-d9e41221e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- End-to-end: process a file → save aligned TSV ---\n",
    "def align_and_save(src_path: Path, out_path: Path):\n",
    "    df = pd.read_csv(src_path, sep=\"\\t\")\n",
    "    assert {\"Record Number\",\"Token\",\"BIO_Tag\"}.issubset(df.columns), \"Missing required columns.\"\n",
    "    label2id, _ = build_label_maps(df)\n",
    "\n",
    "    aligned_rows = []\n",
    "    for _, g in df.groupby(\"Record Number\", sort=False):\n",
    "        aligned_rows.extend(tokenize_align_group(g, label2id))\n",
    "\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    pd.DataFrame(aligned_rows).to_csv(out_path, sep=\"\\t\", index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved aligned TSV → {out_path} | rows: {len(aligned_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db9fa0fb-0697-4ab4-9172-7b2bd81ec97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved aligned TSV → processed_data/aligned_train_subwords.tsv | rows: 124489\n",
      "Saved aligned TSV → processed_data/aligned_val_subwords.tsv | rows: 13560\n"
     ]
    }
   ],
   "source": [
    "# --- Run for train/val (edit paths if needed) ---\n",
    "align_and_save(\n",
    "    Path(\"processed_data/Tagged_Titles_Train_train_with_BIO.tsv\"),\n",
    "    Path(\"processed_data/aligned_train_subwords.tsv\")\n",
    ")\n",
    "\n",
    "align_and_save(\n",
    "    Path(\"processed_data/Tagged_Titles_Train_val_with_BIO.tsv\"),\n",
    "    Path(\"processed_data/aligned_val_subwords.tsv\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
